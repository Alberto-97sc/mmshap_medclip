{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alberto-97sc/mmshap_medclip/blob/others-clips-version/notebooks/02_rclip_classification_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RClip + SHAP: Clasificaci√≥n M√©dica con Balance Multimodal\n",
        "\n",
        "Este notebook demuestra:\n",
        "- **Clasificaci√≥n de im√°genes m√©dicas** usando RClip\n",
        "- **An√°lisis de explicabilidad** con SHAP\n",
        "- **Medici√≥n del balance multimodal** (TScore/IScore)\n",
        "- **Visualizaci√≥n de mapas de calor** para parches de imagen y tokens de texto\n",
        "\n",
        "Dataset: **ROCO** (Radiology Objects in COntext)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Configuraci√≥n inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n del repositorio\n",
        "REPO_URL = \"https://github.com/Alberto-97sc/mmshap_medclip.git\"\n",
        "LOCAL_DIR = \"/content/mmshap_medclip\"\n",
        "BRANCH = \"others-clips-version\"  # Rama con RClip\n",
        "\n",
        "%cd /content\n",
        "import os, shutil, subprocess, sys\n",
        "\n",
        "if not os.path.isdir(f\"{LOCAL_DIR}/.git\"):\n",
        "    # No est√° clonado a√∫n\n",
        "    !git clone $REPO_URL $LOCAL_DIR\n",
        "else:\n",
        "    # Ya existe: actualiza a la √∫ltima versi√≥n del remoto\n",
        "    %cd $LOCAL_DIR\n",
        "    !git fetch origin\n",
        "    !git checkout $BRANCH\n",
        "    !git reset --hard origin/$BRANCH\n",
        "\n",
        "%cd $LOCAL_DIR\n",
        "!git rev-parse --short HEAD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar el paquete en modo editable\n",
        "%pip install -e /content/mmshap_medclip\n",
        "\n",
        "# Dependencias adicionales si son necesarias\n",
        "%pip install matplotlib seaborn pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Carga de datos y modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n para RClip + clasificaci√≥n\n",
        "CFG_PATH = \"/content/mmshap_medclip/configs/roco_classification_rclip.yaml\"\n",
        "\n",
        "from mmshap_medclip.io_utils import load_config\n",
        "from mmshap_medclip.devices import get_device\n",
        "from mmshap_medclip.registry import build_dataset, build_model\n",
        "\n",
        "# Cargar configuraci√≥n\n",
        "cfg = load_config(CFG_PATH)\n",
        "device = get_device()\n",
        "print(f\"üñ•Ô∏è Dispositivo: {device}\")\n",
        "\n",
        "# Cargar dataset ROCO\n",
        "print(\"üìÅ Cargando dataset ROCO...\")\n",
        "dataset = build_dataset(cfg[\"dataset\"])\n",
        "print(f\"‚úÖ Dataset cargado: {len(dataset)} muestras\")\n",
        "\n",
        "# Cargar modelo RClip\n",
        "print(\"ü§ñ Cargando modelo RClip...\")\n",
        "model = build_model(cfg[\"model\"], device=device)\n",
        "print(\"‚úÖ Modelo RClip cargado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir clases para clasificaci√≥n m√©dica\n",
        "class_names = [\n",
        "    \"Chest X-Ray\",\n",
        "    \"Brain MRI\", \n",
        "    \"Abdominal CT Scan\",\n",
        "    \"Ultrasound\",\n",
        "    \"OPG\",  # Orthopantomogram\n",
        "    \"Mammography\",\n",
        "    \"Bone X-Ray\",\n",
        "    \"Cardiac MRI\",\n",
        "    \"Pulmonary CT\",\n",
        "    \"Spinal MRI\"\n",
        "]\n",
        "\n",
        "print(f\"üè∑Ô∏è Clases definidas: {len(class_names)}\")\n",
        "for i, clase in enumerate(class_names):\n",
        "    print(f\"  {i+1}. {clase}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Ejemplo 1: Clasificaci√≥n simple (sin SHAP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mmshap_medclip.tasks.classification import run_classification_one\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Seleccionar una muestra del dataset\n",
        "muestra_idx = 266  # Cambiar por cualquier √≠ndice v√°lido\n",
        "sample = dataset[muestra_idx]\n",
        "image = sample['image']\n",
        "caption = sample['text']\n",
        "\n",
        "print(f\"üìã Muestra {muestra_idx}:\")\n",
        "print(f\"Caption original: {caption[:100]}...\")\n",
        "\n",
        "# Mostrar la imagen\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Muestra {muestra_idx} - ROCO Dataset\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clasificaci√≥n r√°pida sin SHAP\n",
        "print(\"‚ö° Ejecutando clasificaci√≥n r√°pida (sin SHAP)...\")\n",
        "res_simple = run_classification_one(\n",
        "    model, image, class_names, device, \n",
        "    explain=False  # Sin explicabilidad para mayor velocidad\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ Resultados de clasificaci√≥n:\")\n",
        "print(f\"Clase predicha: {res_simple['predicted_class']}\")\n",
        "print(f\"Confianza: {res_simple['probabilities'].max():.2%}\")\n",
        "\n",
        "print(f\"\\nüìä Todas las probabilidades:\")\n",
        "for clase, prob in zip(class_names, res_simple['probabilities']):\n",
        "    bar = \"‚ñà\" * int(prob * 20)  # Barra visual\n",
        "    print(f\"  {clase:<20}: {prob:.2%} {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Ejemplo 2: Clasificaci√≥n con SHAP y Balance Multimodal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üî¨ Ejecutando clasificaci√≥n con SHAP (esto puede tomar varios minutos)...\")\n",
        "res_shap = run_classification_one(\n",
        "    model, image, class_names, device, \n",
        "    explain=True,  # Con explicabilidad SHAP\n",
        "    plot=True      # Generar mapas de calor\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ Resultados con SHAP:\")\n",
        "print(f\"Clase predicha: {res_shap['predicted_class']}\")\n",
        "print(f\"Confianza: {res_shap['probabilities'].max():.2%}\")\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è Balance Multimodal:\")\n",
        "print(f\"TScore (Text Score): {res_shap['tscore']:.2%}\")\n",
        "print(f\"IScore (Image Score): {res_shap['iscore']:.2%}\")\n",
        "\n",
        "# Interpretaci√≥n del balance\n",
        "if res_shap['tscore'] > 0.6:\n",
        "    balance_msg = \"üî§ Modelo se enfoca m√°s en el TEXTO\"\n",
        "elif res_shap['iscore'] > 0.6:\n",
        "    balance_msg = \"üñºÔ∏è Modelo se enfoca m√°s en la IMAGEN\"\n",
        "else:\n",
        "    balance_msg = \"‚öñÔ∏è Balance equilibrado entre texto e imagen\"\n",
        "    \n",
        "print(f\"Interpretaci√≥n: {balance_msg}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar mapa de calor si est√° disponible\n",
        "if 'fig' in res_shap:\n",
        "    print(\"üó∫Ô∏è Mapa de calor con importancia de parches y tokens:\")\n",
        "    display(res_shap['fig'])\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se gener√≥ mapa de calor\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Conclusiones\n",
        "\n",
        "Este notebook ha demostrado:\n",
        "\n",
        "1. **‚úÖ Clasificaci√≥n m√©dica**: RClip puede clasificar im√°genes m√©dicas en m√∫ltiples categor√≠as\n",
        "2. **‚úÖ Explicabilidad con SHAP**: Podemos entender qu√© partes de la imagen y texto son importantes\n",
        "3. **‚úÖ Balance multimodal**: Medimos si el modelo se enfoca m√°s en texto o imagen\n",
        "4. **‚úÖ Visualizaci√≥n**: Mapas de calor muestran la importancia espacial y textual\n",
        "\n",
        "### Pr√≥ximos pasos:\n",
        "- Evaluar en m√°s muestras del dataset\n",
        "- Comparar con otros modelos CLIP\n",
        "- An√°lisis de casos espec√≠ficos por tipo de imagen m√©dica\n",
        "- Optimizaci√≥n de hiperpar√°metros para mejor balance multimodal\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
